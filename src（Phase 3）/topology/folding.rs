// COPYRIGHT (C) 2025 M-Patek. ALL RIGHTS RESERVED.

use super::tensor::HyperTensor;
use crate::phase3::core::affine::AffineTuple;
use crate::phase3::core::algebra::ClassGroupElement;
use rug::Integer;
use std::collections::HashMap;

impl HyperTensor {
    // [API CHANGE]: å…¬å¼€çš„è®¡ç®—å…¥å£ï¼Œé»˜è®¤ä½¿ç”¨è‡ªç„¶åº [0, 1, 2, ...]
    pub fn calculate_global_root(&mut self) -> Result<AffineTuple, String> {
        // æ„å»ºè‡ªç„¶åº: [0, 1, 2, ... D-1]
        let default_order: Vec<usize> = (0..self.dimensions).collect();
        
        // æ³¨æ„ï¼šè¿™é‡Œçš„ cached_root åº”å½“åŸºäºæ–°çš„æŠ˜å é€»è¾‘å¤±æ•ˆæ—¶æ¸…é™¤
        if let Some(ref root) = self.cached_root {
             // return Ok(root.clone()); // æš‚æ—¶ç¦ç”¨ç¼“å­˜ä»¥ç¡®ä¿ç»´åº¦ç½®æ¢æµ‹è¯•çš„æ­£ç¡®æ€§
        }

        let root = self.compute_root_internal(&default_order)?;
        // self.cached_root = Some(root.clone());
        Ok(root)
    }

    // [API CHANGE]: å†…éƒ¨è®¡ç®—ç°åœ¨æ”¯æŒâ€œç»´åº¦ç½®æ¢â€
    pub fn compute_root_internal(&self, dim_order: &[usize]) -> Result<AffineTuple, String> {
        // [Phase 1]: Micro-Fold (Time Aggregation - Non-Commutative)
        let flat_data = self.reconstruct_spatial_snapshot()?;

        // [Phase 2]: Macro-Fold (Spatial Aggregation - Commutative)
        // ä»æ·±åº¦ 0 å¼€å§‹é€’å½’ï¼Œä¾ç…§ dim_order æŒ‡å®šçš„é¡ºåº
        let root = self.fold_sparse(0, dim_order, &flat_data)?;
        Ok(root)
    }

    /// ğŸ› ï¸ ä»æ—¶é—´çº¿é‡å»ºç©ºé—´å¿«ç…§
    fn reconstruct_spatial_snapshot(&self) -> Result<HashMap<Vec<usize>, AffineTuple>, String> {
        let mut snapshot = HashMap::new();
        let one = Integer::from(1);
        let identity_q = ClassGroupElement::identity(&self.discriminant);

        for (coord, time_tree) in &self.data {
            // [Time Collapse]: è¿™ä¸€æ­¥ä½“ç°äº†å› æœå¾‹ (éäº¤æ¢)
            let cell_time_root = time_tree.root(&self.discriminant)?;

            // [Sparse Optimization]
            if cell_time_root.p_factor != one {
                snapshot.insert(coord.clone(), cell_time_root);
            } else {
                if cell_time_root.q_shift != identity_q {
                     snapshot.insert(coord.clone(), cell_time_root);
                }
            }
        }
        Ok(snapshot)
    }

    // æ ¸å¿ƒç®—æ³•ï¼šæ”¯æŒç»´åº¦ç½®æ¢çš„ç¨€ç–æŠ˜å 
    fn fold_sparse(
        &self,
        depth: usize, // å½“å‰é€’å½’æ·±åº¦ (0..D)
        dim_order: &[usize], // ç»´åº¦æŠ˜å é¡ºåº
        relevant_data: &HashMap<Vec<usize>, AffineTuple>
    ) -> Result<AffineTuple, String> {
        if relevant_data.is_empty() {
             return Ok(AffineTuple::identity(&self.discriminant));
        }

        if depth == self.dimensions {
             return Ok(AffineTuple::identity(&self.discriminant));
        }

        // [CRITICAL CHANGE]: è·å–å½“å‰å±‚éœ€è¦æŠ˜å çš„â€œç‰©ç†ç»´åº¦â€
        // è¿™å…è®¸äº† Fold(X->Y) å’Œ Fold(Y->X) çš„è‡ªç”±åˆ‡æ¢
        let target_dim = dim_order[depth];

        // Grouping: æŒ‰ target_dim çš„åæ ‡å€¼åˆ†ç»„
        let mut groups: HashMap<usize, HashMap<Vec<usize>, AffineTuple>> = HashMap::new();
        for (coord, tuple) in relevant_data {
            if target_dim >= coord.len() { continue; }
            let idx = coord[target_dim];
            groups.entry(idx)
                .or_insert_with(HashMap::new)
                .insert(coord.clone(), tuple.clone());
        }

        let mut layer_agg = AffineTuple::identity(&self.discriminant);
        let mut sorted_indices: Vec<usize> = groups.keys().cloned().collect();
        sorted_indices.sort(); 

        for idx in sorted_indices {
            let sub_map = groups.get(&idx).unwrap();
            
            // Recurse: æ·±åº¦ +1
            let sub_result = self.fold_sparse(depth + 1, dim_order, sub_map)?;
            
            // [BOUNDARY CHECK]: å¿…é¡»ä½¿ç”¨ commutative_merge
            // åªæœ‰é˜¿è´å°”ç¾¤çš„èšåˆæ‰èƒ½ä¿è¯ Fold(Order_A) == Fold(Order_B)
            layer_agg = layer_agg.commutative_merge(&sub_result, &self.discriminant)?;
        }

        Ok(layer_agg)
    }
}
